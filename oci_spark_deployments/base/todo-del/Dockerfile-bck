ARG BASE_CONTAINER=centos:centos7
FROM $BASE_CONTAINER AS builder

#
ARG MAVEN_VERSION=3.6.3
ENV USER_HOME_DIR="/root"
ENV BASE_URL=https://apache.osuosl.org/maven/maven-3/${MAVEN_VERSION}/binaries
ENV MAVEN_HOME /opt/maven
ENV MAVEN_CONFIG "$USER_HOME_DIR/.m2"
RUN export MAVEN_OPTS="-Xmx10g -XX:ReservedCodeCacheSize=x4g"

#
ARG JAVA_MAJOR_VERSION=11
#ARG JAVA_MAJOR_VERSION=1.8.0
ENV JDK_NAME=java-${JAVA_MAJOR_VERSION}-openjdk-devel
#ENV JDK_NAME=java-1.8.0-openjdk-devel

#
ENV SPARK_VERSION=3.2.1
ENV HADOOP_VERSION=3.2
ENV HADOOP_VERSION_FULL=3.2.2
ENV LIVY_VERSION=0.8.0-incubating


ENV PATH $SPARK_HOME/bin/:$LIVY_HOME/bin:$PATH

USER root

RUN yum clean all
RUN yum  update  -y

RUN yum install epel-release -y
RUN yum install -y sudo git curl yq jq httpie unzip net-tools  vim wget software-properties-common ssh net-tools ca-certificates
#RUN yum install -y python3 python3-pip python3-numpy python3-matplotlib python3-scipy python3-pandas python3-simpy  python-pip

# java
RUN yum install -y  $JDK_NAME  \
    && echo "securerandom.source=file:/dev/urandom" >> /usr/lib/jvm/jre/lib/security/java.security \
    && yum clean all

## python (python 3.6 is deprecated ... should be py 3.8)
##RUN update-alternatives --install "/usr/bin/python" "python" "$(which python3)" 1
#RUN alternatives --install /usr/bin/python python /usr/bin/python3 10
#RUN alternatives --install /usr/bin/python python /usr/bin/python2 20
#RUN alternatives --display python
#RUN alternatives --auto python
#RUN python3 -m pip install -U pip
#RUN python3 -m pip install -U setuptools

#python
RUN yum -y groupinstall "Development Tools"
RUN yum -y install openssl-devel bzip2-devel libffi-devel xz-devel
RUN gcc --version
RUN wget https://www.python.org/ftp/python/3.8.12/Python-3.8.12.tgz
RUN tar xvf Python-3.8.12.tgz
RUN cd Python-3.8*/ && ./configure --enable-optimizations \
    &&  make altinstall
RUN python3.8 --version


#maven install
RUN mkdir -p /opt/maven /opt/maven/ref
RUN echo ${BASE_URL}/apache-maven-${MAVEN_VERSION}-bin.tar.gz
RUN curl -fsSL -o /tmp/apache-maven.tar.gz ${BASE_URL}/apache-maven-${MAVEN_VERSION}-bin.tar.gz
RUN tar -xzf /tmp/apache-maven.tar.gz -C /opt/maven --strip-components=1
RUN rm -f /tmp/apache-maven.tar.gz
RUN ln -s /opt/maven/bin/mvn /usr/bin/mvn

ENV DEPLOY_FOLDER=/build/deploy
ENV SPARK_JARS=$DEPLOY_FOLDER/3rdparty-jars
ENV OCI_JARS=$DEPLOY_FOLDER/oci-jars
ENV STREAMING_JARS=$DEPLOY_FOLDER/streaming-jars
RUN mkdir -p $SPARK_JARS
RUN mkdir -p $OCI_JARS
RUN mkdir -p $STREAMING_JARS

WORKDIR /build

#build spark
RUN git clone https://github.com/apache/spark.git
WORKDIR /build/spark
# git checkout v3.1.1
RUN git checkout v${SPARK_VERSION}
RUN ./dev/make-distribution.sh --tgz -Phadoop-${HADOOP_VERSION}  -Dhadoop.version=${HADOOP_VERSION_FULL} -Phive -Pkubernetes -Pscala-2.12
RUN ls -alt  /build/spark/spark-3.2.1-bin-3.2.2.tgz
# copy into /build/deploy
RUN cp  /build/spark/spark-3.2.1-bin-3.2.2.tgz $DEPLOY_FOLDER

#build livy
WORKDIR /build
RUN git clone https://github.com/apache/incubator-livy.git
WORKDIR   /build/incubator-livy
RUN cp -rf /build/incubator-livy/conf  /build/incubator-livy/conf.origin
COPY ../0.8/pom.xml /build/incubator-livy/pom.xml
COPY ../0.8/python-api/pom.xml /build/incubator-livy/python-api/pom.xml
COPY ../0.8/assembly/pom.xml /build/incubator-livy/assembly/pom.xml
COPY ../0.8/coverage/pom.xml /build/incubator-livy/coverage/pom.xml
COPY conf-livy/*  /build/incubator-livy/conf/
RUN mvn clean package -B -V -e \
        -Pspark-3.0 \
        -Pthriftserver \
        -DskipTests \
        -DskipITs \
        -Dmaven.javadoc.skip=true
RUN ls -atl  /build/incubator-livy/assembly/target/apache-livy-0.8.0-incubating-SNAPSHOT-bin.zip
# copy into /build/deploy
RUN cp  /build/incubator-livy/assembly/target/apache-livy-0.8.0-incubating-SNAPSHOT-bin.zip $DEPLOY_FOLDER


# OCI spark3.2.1 hadoop3.1 jar NOTE hadoop is 3.1 not 3.2.2
#https://github.com/oracle/oci-hdfs-connector/releases/download/v3.3.1.0.3.2/oci-hdfs.zip
ENV HDFS_CONNECTOR_DL_URL=https://github.com/oracle/oci-hdfs-connector/releases/download/v3.2.1.3/oci-hdfs.zip
RUN mkdir -p  /build/oci-hdfs-connector/oci && \
    wget $HDFS_CONNECTOR_DL_URL  -P /build/oci-hdfs-connector  && \
    unzip /build/oci-hdfs-connector/*.zip -d /build/oci-hdfs-connector/oci
RUN ls -atl  /build/oci-hdfs-connector/oci
# copy into /build/deploy
RUN cp -r /build/oci-hdfs-connector/oci $DEPLOY_FOLDER/

#spark stream build
RUN mkdir -p /build/spark-streaming-pom
COPY ../spark-streaming-pom.xml /build/spark-streaming-pom/pom.xml
RUN cd /build/spark-streaming-pom && \
        mvn dependency:copy-dependencies -DoutputDirectory=$STREAMING_JARS
RUN ls -atl $STREAMING_JARS

# 3rdparty-jars
WORKDIR $SPARK_JARS
ARG HADOOP_AWS_VERSION=3.2.1
ARG AWS_JAVA_SDK_VERSION=1.12.183
ARG TFRECORD_VERSION=0.3.0
ARG GCS_CONNECTOR_VERSION=2.0.1
ARG BQ_CONNECTOR_VERSION=0.18.1
# tf
ADD https://repo1.maven.org/maven2/com/linkedin/sparktfrecord/spark-tfrecord_2.12/${TFRECORD_VERSION}/spark-tfrecord_2.12-${TFRECORD_VERSION}.jar $SPARK_JARS
#gcp
ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-${GCS_CONNECTOR_VERSION}.jar $SPARK_JARS
ADD https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/${BQ_CONNECTOR_VERSION}/spark-bigquery-with-dependencies_2.12-${BQ_CONNECTOR_VERSION}.jar $SPARK_JARS
#azure
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.0/hadoop-azure-3.3.0.jar $SPARK_JARS
ADD https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.5/azure-storage-8.6.5.jar $SPARK_JARS
#jetty
ADD https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.26.v20200117/jetty-util-ajax-9.4.26.v20200117.jar $SPARK_JARS
ADD https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.26.v20200117/jetty-util-9.4.26.v20200117.jar $SPARK_JARS
#oci
ADD https://repo.maven.apache.org/maven2/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar $SPARK_JARS
#aws
#ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar $SPARK_JARS
#ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VERSION}/aws-java-sdk-bundle-${AWS_SDK_VERSION}.jar $SPARK_JARS
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION_FULL}/hadoop-aws-${HADOOP_VERSION_FULL}.jar $SPARK_JARS
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_VERSION}.jar $SPARK_JARS


RUN ls -altR  /build/spark/spark-3.2.1-bin-3.2.2.tgz
RUN ls -altR  /build/incubator-livy/assembly/target/apache-livy-0.8.0-incubating-SNAPSHOT-bin.zip
RUN ls -altR  $DEPLOY_FOLDER

RUN python3.8 --version

COPY *.sh /
RUN chmod + /*.sh

CMD ["/bin/bash", "/statup_base.sh"]

#
#ARG BASE_CONTAINER=centos:centos7
#FROM $BASE_CONTAINER
#
##
#ENV  SPARK_MASTER_PORT=7077
#ENV  SPARK_MASTER_WEBUI_PORT=8080
#ENV  SPARK_WORKER_WEBUI_PORT=8080
#ENV  SPARK_WORKER_PORT=7000
#ENV  PYTHONHASHSEED=1
#
##
#ENV SPARK_HOME=/opt/spark
#ENV HADOOP_HOME /opt/hadoop
#ENV LIVY_HOME /opt/livy
#
##
#ENV LIVY_CONF_DIR=/opt/livy/conf
#ENV LIVY_LOG_DIR=/var/log/livy
#ENV LIVY_LOG_FILE=/var/log/livy/livy--server.out
##
#ENV SPARK_LOG_DIR=/home/centos/logs
#ENV SPARK_MASTER_LOG=/home/centos/logs/spark-master.out
#ENV SPARK_WORKER_LOG=/home/centos/logs/spark-worker.out
#
#ENV PATH $SPARK_HOME/bin/:$LIVY_HOME/bin:$PATH
##
#ARG JAVA_MAJOR_VERSION=11
#ENV JDK_NAME=java-${JAVA_MAJOR_VERSION}-openjdk-devel
#
#
#USER root
#
#RUN yum clean all
#RUN yum  update  -y
#RUN yum install epel-release -y
#
#RUN yum install -y sudo git curl yq jq httpie unzip net-tools  vim wget software-properties-common ssh net-tools ca-certificates
##RUN yum install -y python3 python3-pip python3-numpy python3-matplotlib python3-scipy python3-pandas python3-simpy  python-pip
#
## java
#RUN yum install -y  $JDK_NAME  \
#    && echo "securerandom.source=file:/dev/urandom" >> /usr/lib/jvm/jre/lib/security/java.security \
#    && yum clean all \

# python (python 3.6 is deprecated ... should be py 3.8)
#RUN yum -y groupinstall "Development Tools"
# xz-devel
#RUN yum -y install openssl-devel bzip2-devel libffi-devel make gcc xz-devel
#RUN gcc --version



#RUN ln -sf /usr/local/bin/python3.8 /usr/local/bin/python
#FROM base
#COPY --from=builder /usr/local/lib/python3.8/site-packages/ /usr/local/lib/python3.8/site-packages/
#COPY --from=builder /usr/local/bin/ /usr/local/bin/
#COPY --from=builder /usr/local/lib/python3.8/site-packages/ /usr/local/lib/python3.8/site-packages/

#
##RUN update-alternatives --install "/usr/bin/python" "python" "$(which python3)" 1
#RUN alternatives --install /usr/bin/python python /usr/bin/python3 10
#RUN alternatives --install /usr/bin/python python /usr/bin/python2 20
#RUN alternatives --display python
#RUN alternatives --auto python
#RUN python3 -m pip install -U pip
#RUN python3 -m pip install -U setuptools
#
#COPY *.sh /
#RUN chmod + /*.sh
#
#CMD ["/bin/bash", "/statup_base.sh"]

#
#
## JAVA_APP_DIR is used by run-java.sh for finding the binaries
#ENV JAVA_APP_DIR=/code
#ENV JAVA_HOME /etc/alternatives/jre
#
#ENV AWS_SDK_VERSION=1.11.199
#
#ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.2-src.zip
## OCI spark3.2.1 hadoop3.1
##https://github.com/oracle/oci-hdfs-connector/releases/download/v3.3.1.0.3.2/oci-hdfs.zip
#ENV HDFS_CONNECTOR_DL_URL=https://github.com/oracle/oci-hdfs-connector/releases/download/v3.2.1.3/oci-hdfs.zip
#ENV JDBC_JAR_URL=https://repo.maven.apache.org/maven2/com/oracle/database/jdbc/ojdbc8/21.3.0.0/ojdbc8-21.3.0.0.jar
#ENV HADOOP_AWS_URL=https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION_FULL}/hadoop-aws-${HADOOP_VERSION_FULL}.jar
#ENV AWS_JAVA_SDK_BUNDLE_URL=https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VERSION}/aws-java-sdk-bundle-${AWS_SDK_VERSION}.jar
#ENV SPARK_URL="https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"
#ENV HADOOP_URL=http://apache.mirrors.tds.net/hadoop/common/hadoop-${HADOOP_VERSION_FULL}/hadoop-${HADOOP_VERSION_FULL}.tar.gz
#ARG HADOOP_AWS_VERSION=3.2.1
#ARG AWS_JAVA_SDK_VERSION=1.11.874
#ARG TFRECORD_VERSION=0.3.0
#ARG GCS_CONNECTOR_VERSION=2.0.1
#ARG BQ_CONNECTOR_VERSION=0.18.1
## Add HADOOP_AWS_JAR and AWS_JAVA_SDK
#ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_VERSION}.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/com/linkedin/sparktfrecord/spark-tfrecord_2.12/${TFRECORD_VERSION}/spark-tfrecord_2.12-${TFRECORD_VERSION}.jar /opt/spark/jars
#ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-${GCS_CONNECTOR_VERSION}.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/com/google/cloud/spark/spark-bigquery-with-dependencies_2.12/${BQ_CONNECTOR_VERSION}/spark-bigquery-with-dependencies_2.12-${BQ_CONNECTOR_VERSION}.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.3.0/hadoop-azure-3.3.0.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.5/azure-storage-8.6.5.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.26.v20200117/jetty-util-ajax-9.4.26.v20200117.jar /opt/spark/jars
#ADD https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.26.v20200117/jetty-util-9.4.26.v20200117.jar /opt/spark/jars
#
#
##oci
#RUN mkdir -p /build/spark-streaming-pom
#COPY spark-streaming-pom.xml /build/spark-streaming-pom/pom.xml
#RUN cd /build/spark-streaming-pom && \
#        mvn dependency:copy-dependencies -DoutputDirectory=$SPARK_HOME/jars
#
#
#
#
#
#
## spark
#RUN wget --no-verbose -O apache-spark.tgz  $SPARK_URL \
#&& mkdir -p /opt/spark \
#&& tar -xf apache-spark.tgz -C /opt/spark --strip-components=1 \
#&& rm apache-spark.tgz
#
##aws hadoop/s3
#RUN wget $HADOOP_AWS_URL -P $SPARK_HOME/jars/
#RUN wget $AWS_JAVA_SDK_BUNDLE_URL -P $SPARK_HOME/jars/
##OCI
#RUN mkdir -p /tmp/oci/oci-hdfs && \
#    cd  /tmp/oci && \
#    wget $HDFS_CONNECTOR_DL_URL  -P /tmp/oci  && \
#    unzip /tmp/oci/*.zip -d /tmp/oci/oci-hdfs   && \
#    cp /tmp/oci/oci-hdfs/lib/*.jar /opt/spark/jars/ && \
#    cp /tmp/oci/oci-hdfs/third-party/lib/*.jar $SPARK_HOME/jars
#
##RUN mv $SPARK_HOME/jars/jsr305-3.0.0.jar $SPARK_HOME/jars/jsr305-3.0.0.jar.original
#
##OCI JDBC
## resolve duplication
##RUN mv $SPARK_HOME/jars/jsr305-3.0.0.jar $SPARK_HOME/jars/jsr305-3.0.0.jar.original
#RUN wget $JDBC_JAR_URL  -P $SPARK_HOME/jars/
#
##copy additional files
#COPY 3rdparty/jars ${SPARK_HOME}/jars
#
##  docker logs
#RUN mkdir -p $SPARK_LOG_DIR && touch $SPARK_MASTER_LOG && touch $SPARK_WORKER_LOG && ln -sf /dev/stdout $SPARK_MASTER_LOG && ln -sf /dev/stdout $SPARK_WORKER_LOG
#
## home centos
##WORKDIR /home/centos
##
##add livy into base
##COPY apache-livy-0.8.0-incubating-SNAPSHOT-bin.zip /home/centos
###RUN unzip /home/centos/apache-livy-0.8.0-incubating-SNAPSHOT-bin.zip
###COPY conf-livy /home/centos/apache-livy-0.8.0-incubating-SNAPSHOT-bin/conf
###RUN mv /home/centos/apache-livy-0.8.0-incubating-SNAPSHOT-bin /opt/livy
##COPY ./livy /opt/livy
##RUN chmod +x   /opt/livy/bin/livy-server
##RUN mkdir -p $LIVY_LOG_DIR && touch $LIVY_LOG_FILE   && ln -sf /dev/stdout $LIVY_LOG_FILE
##RUN chown centos -R $LIVY_LOG_DIR
##RUN chown centos -R $LIVY_LOG_FILE
#
#WORKDIR /opt
###### is livy requires haddop lib?
##http://apache.mirrors.tds.net/hadoop/common/hadoop-${HADOOP_VERSION_FULL}/hadoop-${HADOOP_VERSION_FULL}.tar.gz
##https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION_FULL}/hadoop-${HADOOP_VERSION_FULL}.tar.gz
##https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
##http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.2/hadoop-${HADOOP_VERSION_FULL}.tar.gz
#RUN wget ${HADOOP_URL} &&  \
#    tar -xzf hadoop-${HADOOP_VERSION_FULL}.tar.gz && \
#    mv hadoop-${HADOOP_VERSION_FULL} $HADOOP_HOME
#
#
#
### Run under user "centos" and prepare for be running
#RUN groupadd -r centos -g 1000 \
#  && useradd -u 1000 -r -g centos -m -d /home/centos -s /sbin/nologin centos \
#  && chmod 755 /home/centos
#
#RUN mkdir -p /code
#RUN mkdir -p /work
#
#RUN chown -R centos /code \
#  && usermod -g root -G `id -g centos` centos \
#  && chmod -R "g+rwX" /code \
#  && chown -R centos:root /code
#
#RUN echo 'centos ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers
#
#RUN chown -R centos /work \
#  && chmod -R "g+rwX" /work \
#  && chown -R centos:root /work
#
##mount code (cronjob), work (jupyter)
#RUN mkdir -p /code
#RUN mkdir -p /work
#
## code
#COPY *.sh /home/centos/
#COPY *.py /home/centos/
#RUN chmod +x /home/centos/*.sh
#
#RUN mkdir -p /opt/spark/work
#RUN chown -R centos:centos  /opt/spark
#RUN chown -R centos:centos  /home/centos
#
## non root user
#WORKDIR /home/centos
#USER centos
#RUN mkdir /home/centos/.jupyter
#COPY jupyter_notebook_config.py /home/centos/.jupyter/.
##COPY spark-defaults.conf /opt/spark/conf/spark-defaults.conf
#
## python
#RUN python3 -V
#RUN python -V
#COPY requirements.txt .
#RUN pip install -r requirements.txt
#
#
#CMD ["/bin/bash", "/home/centos/statup_base.sh"]
